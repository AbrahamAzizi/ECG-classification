{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZ6VDsiOlYMq"
   },
   "outputs": [],
   "source": [
    "# Go to google drive folder\n",
    "%cd /content/drive/MyDrive/AML/Task2\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fix random seed\n",
    "np.random.seed(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVy1-6ov6aoN"
   },
   "source": [
    "**Install BioSPPy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RCx7FuYA6Zxx"
   },
   "outputs": [],
   "source": [
    "!pip install biosppy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBHTJ_6ilLCK"
   },
   "source": [
    "**Import Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IJv9M04FmDOp"
   },
   "outputs": [],
   "source": [
    "df_x_train = pd.read_csv('X_train.csv')\n",
    "df_y_train = pd.read_csv('y_train.csv')\n",
    "df_x_test = pd.read_csv('X_test.csv')\n",
    "df_sample = pd.read_csv('sample.csv')\n",
    "\n",
    "# drop the column id\n",
    "df_x_train = df_x_train.drop('id', axis=1)\n",
    "df_y_train = df_y_train.drop('id', axis=1)\n",
    "df_x_test = df_x_test.drop('id', axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yxzy2uTGsZHi"
   },
   "outputs": [],
   "source": [
    "# to numpy\n",
    "x_train_total = df_x_train.to_numpy(dtype='float32')\n",
    "y_train_total = df_y_train.to_numpy() \n",
    "x_test = df_x_test.to_numpy(dtype='float32') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4CQP5xbu-tu"
   },
   "source": [
    "**Feature extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wyKrAEz5HvlV"
   },
   "outputs": [],
   "source": [
    "import biosppy.signals.ecg as ecg\n",
    "\n",
    "# hyper parameter:\n",
    "th = 0.48 #r-peak segmentation\n",
    "\n",
    "def extract_features(X):\n",
    "  X_features = np.array([])\n",
    "  n_features = 34\n",
    "  for row in X:\n",
    "    # row without Nans\n",
    "    row = row[np.logical_not(np.isnan(row))]\n",
    "    # R-peak location indices\n",
    "    r_peaks = ecg.engzee_segmenter(row, sampling_rate=300, threshold=th)['rpeaks']\n",
    "    if r_peaks.shape == (0,): #TODO: what to do with those samples?\n",
    "      features = np.zeros(n_features) #set all features to zero if no R-peak was found\n",
    "      print(\"No R-peak found in sample: \", X_features.shape[0])\n",
    "    else:\n",
    "      # heartbeat templates\n",
    "      beats = ecg.extract_heartbeats(row, r_peaks, sampling_rate=300, before=0.2, after=0.4)['templates']\n",
    "      beats_var_mean = np.mean(np.std(beats, axis=0))\n",
    "      beats_var_md = np.median(np.std(beats, axis=0))\n",
    "      beats_var_var = np.std(np.std(beats, axis=0))\n",
    "\n",
    "      amplitude_mean = np.mean(np.mean(np.power(beats, 2), axis=1)) #squared\n",
    "      amplitude_md = np.median(np.mean(np.power(beats, 2), axis=1)) #squared\n",
    "      amplitude_var = np.std(np.mean(np.power(beats, 2), axis=1)) #squared\n",
    "\n",
    "      # R-peak: always at T=60\n",
    "      r_mean = np.mean(beats[:, 60])\n",
    "      r_md = np.median(beats[:, 60])\n",
    "      r_var = np.std(beats[:, 60])\n",
    "\n",
    "      # P-peak: smaller peak before R-peak, 40 good?\n",
    "      p_mean = np.mean(np.amax(beats[:, :40], axis=1))\n",
    "      p_md = np.median(np.amax(beats[:, :40], axis=1))\n",
    "      p_var = np.std(np.amax(beats[:, :40], axis=1))\n",
    "      p_position = np.mean(np.argmax(beats[:, :40], axis=1))\n",
    "      p_position_var = np.std(np.argmax(beats[:, :40], axis=1))\n",
    "      if p_position == 40.0:\n",
    "        print(\"P peak earlier\") #TODO: position good?\n",
    "\n",
    "      # Q-drop: before R-peak\n",
    "      q_mean = np.mean(np.amin(beats[:, :60], axis=1))\n",
    "      q_md = np.median(np.amin(beats[:, :60], axis=1))\n",
    "      q_var = np.std(np.amin(beats[:, :60], axis=1))\n",
    "      q_position = np.mean(np.argmin(beats[:, :60], axis=1))\n",
    "      q_position_var = np.std(np.argmin(beats[:, :60], axis=1))\n",
    "\n",
    "      # S-drop: after R-peak\n",
    "      s_mean = np.mean(np.amin(beats[:, 60:], axis=1))\n",
    "      s_md = np.median(np.amin(beats[:, 60:], axis=1))\n",
    "      s_var = np.std(np.amin(beats[:, 60:], axis=1))\n",
    "      s_position = np.mean(np.argmin(beats[:, 60:], axis=1))\n",
    "      s_position_var = np.std(np.argmin(beats[:, 60:], axis=1))\n",
    "\n",
    "      # T-peak: after R-peak, 80 good?\n",
    "      t_mean = np.mean(np.amax(beats[:, 80:], axis=1))\n",
    "      t_md = np.median(np.amax(beats[:, 80:], axis=1))\n",
    "      t_var = np.std(np.amax(beats[:, 80:], axis=1))\n",
    "      t_position = np.mean(np.argmax(beats[:, 80:], axis=1))\n",
    "      t_position_var = np.std(np.argmax(beats[:, 80:], axis=1))\n",
    "      if t_position == 0.0:\n",
    "        print(\"T peak later\") #TODO: position good?\n",
    "\n",
    "      # instantaneous heart rate (bpm)\n",
    "      heart_rate = ecg.ecg(row, 300, show=False)['heart_rate']\n",
    "      if heart_rate.shape == (0,):\n",
    "        hr_mean = 0\n",
    "        hr_md = 0\n",
    "        hr_var = 0\n",
    "        hr_max = 0\n",
    "        hr_min = 0\n",
    "      else:\n",
    "        hr_mean = np.mean(heart_rate) \n",
    "        hr_md = np.median(heart_rate)\n",
    "        hr_var = np.std(heart_rate)\n",
    "        hr_max = np.amax(heart_rate)\n",
    "        hr_min = np.amin(heart_rate)\n",
    "\n",
    "      features = np.array([beats_var_mean, beats_var_md, beats_var_var,\n",
    "                  amplitude_mean, amplitude_md, amplitude_var,\n",
    "                  r_mean, r_md, r_var,\n",
    "                  p_mean, p_md, p_var, p_position, p_position_var,\n",
    "                  q_mean, q_md, q_var, q_position, q_position_var,\n",
    "                  s_mean, s_md, s_var, s_position, s_position_var,\n",
    "                  t_mean, t_md, t_var, t_position, t_position_var,\n",
    "                  hr_mean, hr_md, hr_var, hr_max, hr_min])\n",
    "\n",
    "    if X_features.shape == (0,):\n",
    "      X_features = features\n",
    "    else:\n",
    "      X_features = np.vstack((X_features, features))\n",
    "\n",
    "  # Normalize\n",
    "  X_features = np.array(X_features)\n",
    "  m = np.nanmean(X_features, axis=0)\n",
    "  s = np.nanstd(X_features, axis=0)\n",
    "  X_features = X_features - m\n",
    "  X_features = X_features / s\n",
    "\n",
    "  return X_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58E0SPLbMdzW"
   },
   "outputs": [],
   "source": [
    "x_train_total_features = extract_features(x_train_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4lyOX_HNGTK"
   },
   "source": [
    "# **Cross-Validate**\n",
    "## Instead of model one can use any classifier. Moreover, the best can be stacked many xgboost(more than 10), random forest, and ridge regression as the final classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zA7h1uOQaBCD"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import resample\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "s=0\n",
    "n_splits = 10\n",
    "val_score_sum = 0\n",
    "kf = KFold(n_splits=n_splits)\n",
    "for train_index, test_index in kf.split(x_train_total_features):\n",
    "  x_train, x_val = x_train_total_features[train_index], x_train_total_features[test_index]\n",
    "  y_train, y_val = y_train_total[train_index], y_train_total[test_index]\n",
    "  s += 1\n",
    "  print(\"Split \", s, \" of \", n_splits)\n",
    "\n",
    "\n",
    "  # Class Imbalance\n",
    "\n",
    "  train = np.concatenate((y_train, x_train), axis=1)\n",
    "  y_train = y_train.ravel() #reshape\n",
    "  train_0 = train[y_train == 0]\n",
    "  train_1 = train[y_train == 1]\n",
    "  train_2 = train[y_train == 2]\n",
    "  train_3 = train[y_train == 3]\n",
    "\n",
    "\n",
    "  # upsample minority classes (Class 0 is the biggest class)\n",
    "  train_1_upsampled = resample(train_1,\n",
    "                            replace=True, # sample with replacement\n",
    "                            n_samples=len(train_0)) # match number in majority class\n",
    "\n",
    "  train_2_upsampled = resample(train_2,\n",
    "                            replace=True, # sample with replacement\n",
    "                            n_samples=len(train_0)) # match number in majority class\n",
    "\n",
    "  train_3_upsampled = resample(train_3,\n",
    "                            replace=True, # sample with replacement\n",
    "                            n_samples=len(train_0)) # match number in majority class\n",
    "\n",
    "  train_upsampled = np.concatenate((train_0, train_1_upsampled, train_2_upsampled, train_3_upsampled), axis=0)\n",
    " \n",
    "  y_train_upsampled, x_train_upsampled = np.split(train_upsampled, [1], axis=1) \n",
    "  \n",
    "  y_train_upsampled = y_train_upsampled.ravel() #reshape\n",
    "  \n",
    "  # Classification Model\n",
    "\n",
    "  model = svm.SVC(C=100.0, class_weight='balanced', probability=True, tol=1e-6)\n",
    "\n",
    "  model.fit(x_train_upsampled, y_train_upsampled)\n",
    "\n",
    "  \n",
    "  # Evaluation\n",
    "\n",
    "  #training set\n",
    "  y_train_pred = model.predict(x_train_upsampled)\n",
    "  F1 = f1_score(y_train_upsampled, y_train_pred, average='micro')\n",
    "  print('F1 score of training set: ', F1)\n",
    "\n",
    "  #validation set\n",
    "  y_val_pred = model.predict(x_val)\n",
    "  F1 = f1_score(y_val, y_val_pred, average='micro')\n",
    "  print('F1 score of validation set: ', F1)\n",
    "  val_score_sum += F1\n",
    "\n",
    "val_score_mean = val_score_sum/n_splits\n",
    "print(\"CV score: \", val_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kt8_trqzvPmT"
   },
   "source": [
    "**Fit and predict on whole training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "juXpvgzLCujB"
   },
   "outputs": [],
   "source": [
    "x_test_features = extract_features(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBK5kQCNcykP"
   },
   "source": [
    "**Class Imbalance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "coJVOjxRcxEm"
   },
   "outputs": [],
   "source": [
    "train = np.concatenate((y_train_total, x_train_total_features), axis=1)\n",
    "y_train_total = y_train_total.reshape(y_train_total.shape[0])\n",
    "train_0 = train[y_train_total == 0]\n",
    "train_1 = train[y_train_total == 1]\n",
    "train_2 = train[y_train_total == 2]\n",
    "train_3 = train[y_train_total == 3]\n",
    "\n",
    "\n",
    "# upsample minority classes (Class 0 is the biggest class)\n",
    "train_1_upsampled = resample(train_1,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(train_0)) # match number in majority class\n",
    "\n",
    "train_2_upsampled = resample(train_2,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(train_0)) # match number in majority class\n",
    "\n",
    "train_3_upsampled = resample(train_3,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(train_0)) # match number in majority class\n",
    "\n",
    "train_upsampled = np.concatenate((train_0, train_1_upsampled, train_2_upsampled, train_3_upsampled), axis=0)\n",
    "\n",
    "y_train_upsampled, x_train_upsampled = np.split(train_upsampled, [1], axis=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Chjc4vpdxElt"
   },
   "outputs": [],
   "source": [
    "model.fit(x_train_upsampled, y_train_upsampled)\n",
    "y_pred = model.predict(x_test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQ2JoVBrw70i"
   },
   "source": [
    "**Save to file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "9DA8Ti3Bw-Uj"
   },
   "outputs": [],
   "source": [
    "df_sample.iloc[:,1] = y_pred #replace values in sample file with predictions\n",
    "# print(df_sample.head())\n",
    "df_sample.to_csv('pred.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Task2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
